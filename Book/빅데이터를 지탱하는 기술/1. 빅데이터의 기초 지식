# 1. 빅데이터의 기초 지식

## 1-1 [배경] 빅데이터의 정착

빅데이터(Big Data)

빅데이터의 취급이 어려운 이유는 크게 두 가지
1. 데이터의 분석 방법을 모른다
2. 데이터 처리에 수고와 시간이 걸린다

전통적인 관계형 데이터베이스(RDB)로는 취급할 수 없을 만큼 대량의 데이터가 점차 쌓이게 되었다.  
그렇게 축적된 데이터를 처리하려면 기존과는 다른 구조가 필요했다.

Hadoop은 '다수의 컴퓨터에서 대량의 데이터를 처리하기' 위한 시스템

NoSQL은 전통적인 RDB의 제약을 제거하는 것을 목표로 한 데이터베이스의 총칭
1. 다수의 키와 값을 관련지어 저장하는 '키 밸류 스토어(key-value store/KVS)'
2. JSON과 같은 복잡한 데이터 구조를 저장하는 '도큐멘트 스토어(document store)'
3. 여러 키를 사용하여 높은 확장성을 제공하는 '와이드 칼럼 스토어(wide-column store)'

RDB보다 고속의 읽기, 쓰기가 가능하고 분산 처리에 뛰어나다는 특징  
모여진 데이터를 나중에 집계하는 것이 목적인 Hadoop과는 다르게 NoSQL은 애플리케이션에서 온라인으로 접속하는 데이터베이스

Hadoop + NoSQL → NoSQL 데이터베이스에 기록하고 Hadoop으로 분산 처리하기

일부 기업에서는 이전부터 데이터 분석을 기반으로 하는 '엔터프라이즈 데이터 웨어하우스(enterprise data warehouse/EDW, 또는 데이터 웨어하우스/DWH)'를 도입했다.

데이터 용량을 늘리려면 하드웨어를 교체해야 하는 등 나중에 확장하기가 쉽지 않았다.  
가속도적으로 늘어나는 데이터의 처리는 Hadoop에 맡기고, 비교적 작은 데이터, 또는 중요한 데이터만을 데이터 웨어하우스에 넣는 식으로 사용을 구분하게 되었다.

'여러 컴퓨터에 분산 처리한다'라는 점이 빅데이터의 특징

기존 기술을 이용해서 취급할 수 있는 작은 데이터를 '스몰 데이터(small data)'라고 한다.  
효율적인 스몰 데이터의 처리 방법을 알지 못한 채, 빅데이터 기술만 배워서는 충분하지 않다. 이 둘을 모두 적재적소로 구사하는 것이 이상적이다.

데이터 웨어하우스에 저장된 데이터를 시각화하려는 방법으로 '데이터 디스커버리(data discovery)'가 인기있음  
데이터 디스커버리란 '대화형으로 데이터를 시각화하여 가치 있는 정보를 찾으려고 하는 프로세스'  
데이터 디스커버리는 '셀프서비스용 BI 도구'로 불린다.  
'BI 도구(business intelligence tool)'는 예전부터 데이터 웨어하우스와 조합되어 사용된 경영자용 시각화 시스템으로 대기업의 IT 부서에 의해 도입되는 대규모의 도구다.  
셀프서비스용 BI 도구는 이것을 개인도 도입할 수 있을 정도로 단순화한 것으로, 이로 인해 점차 많은 사람이 데이터를 살펴볼 수 있게 되었다.  

## 1-2 빅데이터 시대의 데이터 분석 기반

빅데이터 기술이 기존의 데이터 웨어하우스와 다른 점은 다수의 분산 시스템을 조합하여 확장성이 뛰어난 데이터 처리 구조를 만든다는 점

### 빅데이터의 기술 - 분산 시스템을 활용해서 데이터를 가공해 나가는 구조

이 책에서 다루는 '빅데이터 기술'이란 분산 시스템을 활용하면서 데이터를 순차적으로 가공해 나가는 일련의 구조

**데이터 파이프라인**  데이터 수집에서 워크플로 관리까지  
일반적으로 차례대로 전달해나가는 데이터로 구성된 시스템을 '데이터 파이프라인(data pipeline)'이라고 한다.  
데이터 파이프라인은 데이터를 모으는 부분부터 시작한다.

'데이터 전송(data transfer)'의 방법은 크게 두 가지
1. 벌크(bulk) 형
2. 스트리밍(streaming) 형

벌크 형은 이미 어딘가에 존재하는 데이터를 정리해 추출하는 방법으로, 데이터베이스와 파일 서버 등에서 정기적으로 데이터를 수집하는 데에 사용한다.

스트리밍 형은 차례차례로 생성되는 데이터를 끊임없이 계속해서 보내는 방법으로 모바일 애플리케이션과 임베디드 장비 등에서 널리 데이터를 수집하는 데 사용된다.

스트리밍 형 방법으로 받은 데이터는 아무래도 실시간으로 처리하고 싶어진다.  
이것을 '스트림 처리(stream processing)'라고 한다.  
예) 시계열 데이터베이스(time-series database)

장기적인 데이터 분석을 위해서는 보다 대량의 데이터를 저장하고 처리하는 데 적합한 분산 시스템이 좋다. 거기에 필요한 것은 스트림 처리가 아닌, 어느 정도 정리된 데이터를 효율적으로 가공하기 위한 '배치 처리(batch processing)' 구조다.

**분산 스토리지** 객체 스토리지, NoSQL 데이터베이스  
수집된 데이터는 '분산 스토리지(distribute storage)'에 저장된다.  
분산 스토리지란 여러 컴퓨터와 디스크로부터 구성된 스토리지 시스템을 말한다.  
'객체 스토리지(object storage)'란 한 덩어리로 모인 데이터에 이름을 부여해서 파일로 저장한다.  
NoSQL 데이터베이스를 분산 스토리지로 사용할 수 있다.

**분산 데이터 처리** 쿼리 엔진, ETL 프로세스  
분산 스토리지에 저장된 데이터를 처리하는 데는 '분산 데이터 처리(distribute data processing)'의 프레임워크가 필요하다.  
분산 데이터 처리의 주 역할은 나중에 분석하기 쉽도록 데이터를 가공해서 그 결과를 외부 데이터베이스에 저장하는 것이다.  

빅데이터를 SQL로 집계할 때는 두 가지 방법이 있다.  
1. 분산 스토리지 상의 데이터를 SQL로 집계하기 위해 '쿼리 엔진(query engine)'을 도입. Hive는 그 한 가지 예인데, 현재는 Hive보다도 고속인 '대화형 쿼리 엔진(interactive query engine)'도 개발됨
2. 외부의 데이터 웨어하우스 제품을 이용하는 것. 이를 위해서는 분산 스토리지에서 추출한 데이터를 데이터 웨어하우스에 적합한 형식으로 변환한다. 이 일련의 절차를 'ETL(extract-transform-load) 프로세스' 라고 한다. 데이터를 추출(extract)하고, 그것을 가공(transform)한 후, 데이터 웨어하우스에 로드(load)한다.

**워크플로 관리**
전체 데이터 파이프라인의 동작을 관리하기 위해서 '워크플로 관리(workflow management)' 기술을 사용한다. 매일 정해진 시간에 배치 처리를 스케줄대로 실행하고, 오류가 발생한 경우에는 관리자에게 통지하는 목적으로 사용된다. 

### 데이터 웨어하우스와 데이터 마트 - 데이터 파이프라인의 기본형

데이터 웨어하우스는 웹 서버나 업무 시스템에서 이용되는 일반적인 RDB와는 달리 '대량의 데이터를 장기 보존하는' 것에 최적화되어 있다. 정리된 데이터를 한 번에 전송하는 것은 뛰어나지만, 소량의 데이터를 자주 쓰고 읽는 데는 적합하지 않다.

업무 시스템을 위한 RDB나 로그 등을 저장하는 파일 서버는 '데이터 소스(data source)'라고 부른다. 거기에 보존된 '로우 데이터(raw data, 원시 데이터)'를 추출하고 필요에 따라 가공한 후 데이터 웨어하우스에 저장하기까지의 흐름이 'ETL 프로세스(ETL Process)'다.

데이터 웨어하우스는 업무에 있어서 중요한 데이터 처리에 사용되기 때문에 아무때나 함부로 사용해 시스템에 과부하를 초래하는 것은 곤란하다. 따라서, 데이터 분석과 같은 목적에 사용하는 경우에는 데이터 웨어하우스에서 필요한 데이터만을 추출하여 '데이터 마트(data mart)'를 구축한다. 데이터 마트는 BI 도구와 조합시키는 형태로 데이터를 시각화하는 데에도 사용된다.

데이터 웨어하우스와 데이터 마트 모두 SQL로 데이터를 집계한다. 따라서, 먼저 테이블 설계를 제대로 정한 후에 데이터를 투입한다. 특히 BI 도구로 데이터를 볼 경우에는 미리 시각화에 적합한 형태로 테이블을 준비해야 한다. 그렇기에 데이터 웨어하우스를 중심으로 하는 파이프라인에서는 테이블 설계와 ETL 프로세스가 중요하다. 

### 데이터 레이크 - 데이터를 그대로 축적

모든 데이터를 원래의 형태로 축적해두고 나중에 그것을 필요에 따라 가공하는 구조가 필요하다. 빅데이터의 세계에서는 여러 곳에서 데이터가 흘러들어 오는 '데이터를 축적하는 호수'에 비유해 데이터의 축적 장소를 '데이터 레이크(data lake)'라고 한다.

구체적으로는 임의의 데이터를 저장할 수 있는 분산 스토리지가 데이터 레이크로 이용된다. 데이터 형식은 자유지만, 대부분의 경우는 CSV나 JSON 등의 범용적인 텍스트 형식이 사용된다.

### 데이터 레이크와 데이터 마트 - 필요한 데이터는 데이터 마트에 정리

데이터 레이크는 단순한 스토리지이며, 그것만으로는 데이터를 가공할 수 없다. 그래서 사용되는 것이 MapReduce 등의 분산 데이터 처리 기술이다. 데이터 분석에 필요한 데이터를 가공, 집계하고, 이것을 데이터 마트로 추출한 후에는 데이터 웨어하우스의 경우처럼 데이터 분석을 진행할 수 있다.

시스템의 구축 및 운용, 자동화 등을 담당하는 '데이터 엔지니어(data engineer)'와 데이터에서 가치있는 정보를 추출하는 '데이터 분석가(data analyst)'는 요구되는 지식뿐만 아니라 사용 도구도 다르다.

수작업으로 데이터를 집계하는 것은 '일회성 데이터 분석'이라는 의미로 '애드 혹 분석(ad hoc analysis)'라고 한다.

새로운 도구와 서비스가 계속해서 개발되고 있지만, 데이터 파이프라인 전체의 기본적인 흐름은 변하지 않는다.
기본적으로는 다음의 두 가지만 파악해두면 크게 문제 될 일 없음
1. 저장할 수 있는 데이터 용량에 제한이 없을 것
2. 데이터를 효율적으로 추출할 수단이 있을 것

복잡한 데이터 분석에서는 먼저 데이터 마트를 구축한 후에 분석하거나 시각화하도록 한다. 특히 시각화에 BI 도구를 사용할 경우는 집계 속도를 높이기 위해 데이터 마트가 필수적이다. 데이터 마트 구축은 배치 처리로 자동화하는 경우가 많기 때문에 그 실행 관리를 위해 워크플로 관리 도구를 사용한다.

데이터 검색 - 대량의 데이터 중에서 조건에 맞는 것을 찾고 싶은 경우. 
시스템에는 실시간 데이터 처리나 검색 엔진을 사용하여 키워드를  찾는 기능이 필요하다.

데이터의 가공 - 업무 시스템의 일부로서 데이터 처리 결과를 이용하고 싶은 경우. 
자동화가 필수적이다. 따라서, 워크플로 관리를 도입하여 꼼꼼하게 테스트를 반복적으로 실행해서 시스템을 구축한다.

데이터 시각화 - 데이터를 시각적으로 봄으로써 알고 싶은 정보를 얻는 경우
통계 분석 소프트웨어나 BI 도구 등으로 그래프를 만들고 거기서 앞으로의 상황을 예측해 의사 결정에 도움이 되도록 하는 경우
시각화를 고속화하려면 데이터 마트가 필요하다.
집계 결과를 대시보드에 정리해서 계속 변화를 감시하고 싶을 때도 데이터 시각화는 필요하다.

컴퓨터 시스템은 기간계 시스템(mission-critical system)과 정보계 시스템(information system)으로 구분

데이터 분석이란 가설을 세우고 그것을 검증하는 '확증적 데이터 분석(confirmatory data analysis)'과 데이터를 보면서 그 의미를 읽어내려고 하는 '탐색적 데이터 분석(exploratory data analysis)'으로 나눌 수 있다.  
전자가 주로 통계학적 모델링에 의한 데이터 분석이라면, 후자는 데이터를 시각화하여 사람의 힘으로 그 의미를 읽는다.

## 1-3 스크립트 언어에 의한 특별 분석과 데이터 프레임

데이터 분석 분야에서 인기있는 언어 : R, Python  
파이썬은 과학 기술 계산 분야에서 오랜 기간 사용되었고 NumPy와 SciPy라는 수치 계산용 라이브러리와 머신러닝의 프레임워크가 충실하다. 데이터 처리 분야에서는 R에서 사용하는 '데이터 프레임'의 모델을 파이썬으로 만든 라이브러리인 pandas를 많이 사용하고 있다.

### 데이터 프레임, 기초 중의 기초 - '배열 안의 배열'로부터 작성

'데이터 프레임(data frame)'은 표 형식의 데이터를 추상화한 객체다.  
스프레드시트에 있어 하나의 시트 또는 데이터베이스에 있어 하나의 테이블을 통째로 하나의 객체로 취급한다고 생각하면 된다.
